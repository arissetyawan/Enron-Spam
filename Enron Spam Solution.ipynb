{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data set from here: http://www.aueb.gr/users/ion/data/enron-spam/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootdir = \"C:\\\\Users\\\\Shantnu\\\\Desktop\\\\Data Sources\\\\Enron Spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam ['enron1', 'enron2', 'enron3', 'enron4', 'enron5', 'enron6'] 0\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron1 ['ham', 'spam'] 1\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron1\\ham [] 3672\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron1\\spam [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron2 ['ham', 'spam'] 1\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron2\\ham [] 4361\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron2\\spam [] 1496\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron3 ['ham', 'spam'] 1\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron3\\ham [] 4012\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron3\\spam [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron4 ['ham', 'spam'] 1\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron4\\ham [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron4\\spam [] 4500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron5 ['ham', 'spam'] 1\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron5\\ham [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron5\\spam [] 3675\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron6 ['ham', 'spam'] 1\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron6\\ham [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron6\\spam [] 4500\n"
     ]
    }
   ],
   "source": [
    "# Loop through all the directories, sub directories and files in the above folder, and print them.\n",
    "\n",
    "# For files, print number of files.\n",
    "\n",
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    print(directories, subdirs, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C:\\\\Users\\\\Shantnu\\\\Desktop\\\\Data Sources\\\\Enron Spam\\\\enron1', 'ham')\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron1\n",
      "ham\n"
     ]
    }
   ],
   "source": [
    "print(os.path.split(\"C:\\\\Users\\\\Shantnu\\\\Desktop\\\\Data Sources\\\\Enron Spam\\\\enron1\\\\ham\"))\n",
    "print(os.path.split(\"C:\\\\Users\\\\Shantnu\\\\Desktop\\\\Data Sources\\\\Enron Spam\\\\enron1\\\\ham\")[0])\n",
    "print(os.path.split(\"C:\\\\Users\\\\Shantnu\\\\Desktop\\\\Data Sources\\\\Enron Spam\\\\enron1\\\\ham\")[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron1\\ham [] 3672\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron1\\spam [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron2\\ham [] 4361\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron2\\spam [] 1496\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron3\\ham [] 4012\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron3\\spam [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron4\\ham [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron4\\spam [] 4500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron5\\ham [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron5\\spam [] 3675\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron6\\ham [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron6\\spam [] 4500\n"
     ]
    }
   ],
   "source": [
    "# Same as before, but only print the ham and spam folders\n",
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        print(directories, subdirs, len(files))\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        print(directories, subdirs, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: christmas tree farm pictures\n",
      "\n",
      "Subject: dobmeos with hgh my energy level has gone up ! stukm\n",
      "introducing\n",
      "doctor - formulated\n",
      "hgh\n",
      "human growth hormone - also called hgh\n",
      "is referred to in medical science as the master hormone . it is very plentiful\n",
      "when we are young , but near the age of twenty - one our bodies begin to produce\n",
      "less of it . by the time we are forty nearly everyone is deficient in hgh ,\n",
      "and at eighty our production has normally diminished at least 90 - 95 % .\n",
      "advantages of hgh :\n",
      "- increased muscle strength\n",
      "- loss in body fat\n",
      "- increased bone density\n",
      "- lower blood pressure\n",
      "- quickens wound healing\n",
      "- reduces cellulite\n",
      "- improved vision\n",
      "- wrinkle disappearance\n",
      "- increased skin thickness texture\n",
      "- increased energy levels\n",
      "- improved sleep and emotional stability\n",
      "- improved memory and mental alertness\n",
      "- increased sexual potency\n",
      "- resistance to common illness\n",
      "- strengthened heart muscle\n",
      "- controlled cholesterol\n",
      "- controlled mood swings\n",
      "- new hair growth and color restore\n",
      "read\n",
      "more at this website\n",
      "unsubscribe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ham_list = []\n",
    "spam_list = []\n",
    "\n",
    "# Same as before, but this time, read the files, and append them to the ham and spam list\n",
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        for filename in files:      \n",
    "            with open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                ham_list.append(data)\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        for filename in files:\n",
    "            with open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                spam_list.append(data)\n",
    "\n",
    "\n",
    "print(ham_list[0])\n",
    "print(spam_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3 Encodings: http://python-notes.curiousefficiency.org/en/latest/python3/text_file_processing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': True, 'brown': True, 'fox': True, 'quick': True, 'the': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a function , that when passed in words, will return a dictionary of the form\n",
    "\n",
    "# {Word1: True, Word2: True, Words3: True}\n",
    "\n",
    "# Removing stop words is optional\n",
    "\n",
    "def create_word_features(words):\n",
    "    my_dict = dict( [ (word, True) for word in words] )\n",
    "    return my_dict\n",
    "\n",
    "create_word_features([\"the\", \"quick\", \"brown\", \"quick\", \"a\", \"fox\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'Subject': True, 'pictures': True, ':': True, 'farm': True, 'christmas': True, 'tree': True}, 'ham')\n",
      "({'production': True, 'read': True, 'is': True, 'sexual': True, 'improved': True, 'age': True, 'strength': True, '.': True, 'medical': True, 'healing': True, 'website': True, 'introducing': True, 'hormone': True, 'color': True, 'master': True, 'controlled': True, 'levels': True, 'restore': True, 'as': True, 'cellulite': True, '-': True, 'least': True, 'energy': True, 'bone': True, 'near': True, '90': True, 'diminished': True, 'eighty': True, '!': True, 'mood': True, 'our': True, 'level': True, ',': True, 'twenty': True, 'to': True, 'nearly': True, 'time': True, 'human': True, '%': True, 'this': True, 'called': True, 'cholesterol': True, 'referred': True, 'normally': True, 'illness': True, 'emotional': True, 'one': True, 'of': True, 'stukm': True, 'dobmeos': True, 'bodies': True, 'hgh': True, 'wrinkle': True, 'but': True, 'deficient': True, 'skin': True, 'texture': True, 'memory': True, 'pressure': True, 'quickens': True, 'muscle': True, 'produce': True, 'lower': True, 'body': True, 'advantages': True, 'heart': True, 'unsubscribe': True, 'begin': True, 'in': True, 'Subject': True, 'resistance': True, 'strengthened': True, 'thickness': True, 'by': True, 'growth': True, 'stability': True, 'increased': True, 'density': True, 'disappearance': True, 'loss': True, 'and': True, 'everyone': True, 'new': True, 'young': True, 'are': True, 'vision': True, 'blood': True, 'hair': True, 'very': True, 'at': True, 'sleep': True, 'forty': True, 'doctor': True, 'the': True, 'when': True, 'gone': True, 'mental': True, ':': True, 'alertness': True, 'up': True, 'wound': True, 'potency': True, 'it': True, 'also': True, 'more': True, 'formulated': True, 'common': True, 'my': True, 'has': True, 'science': True, '95': True, 'plentiful': True, 'fat': True, 'reduces': True, 'we': True, 'with': True, 'swings': True, 'less': True}, 'spam')\n"
     ]
    }
   ],
   "source": [
    "ham_list = []\n",
    "spam_list = []\n",
    "\n",
    "# Same as before, but this time:\n",
    "\n",
    "# 1. Break the sentences into words using word_tokenize\n",
    "# 2. Use the create_word_features() function you just wrote\n",
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        for filename in files:      \n",
    "            with open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                \n",
    "                # The data we read is one big string. We need to break it into words.\n",
    "                words = word_tokenize(data)\n",
    "                \n",
    "                ham_list.append((create_word_features(words), \"ham\"))\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        for filename in files:\n",
    "            with open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                \n",
    "                # The data we read is one big string. We need to break it into words.\n",
    "                words = word_tokenize(data)\n",
    "                \n",
    "                spam_list.append((create_word_features(words), \"spam\"))\n",
    "print(ham_list[0])\n",
    "print(spam_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33716\n"
     ]
    }
   ],
   "source": [
    "combined_list = ham_list + spam_list\n",
    "print(len(combined_list))\n",
    "\n",
    "random.shuffle(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33716\n",
      "23601\n",
      "10115\n"
     ]
    }
   ],
   "source": [
    "# Create a test and train section.\n",
    "\n",
    "# 70% of the data is training. 30% is test\n",
    "\n",
    "training_part = int(len(combined_list) * .7)\n",
    "\n",
    "print(len(combined_list))\n",
    "\n",
    "training_set = combined_list[:training_part]\n",
    "\n",
    "test_set =  combined_list[training_part:]\n",
    "\n",
    "print (len(training_set))\n",
    "print (len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  98.50716757291151\n"
     ]
    }
   ],
   "source": [
    "# Create the Naive Bayes filter\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# Find the accuracy, using the test data\n",
    "\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
    "\n",
    "print(\"Accuracy is: \", accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   enron = True              ham : spam   =   3588.6 : 1.0\n",
      "                     hpl = True              ham : spam   =    577.0 : 1.0\n",
      "                     php = True             spam : ham    =    416.2 : 1.0\n",
      "                     713 = True              ham : spam   =    326.3 : 1.0\n",
      "                  louise = True              ham : spam   =    299.2 : 1.0\n",
      "                     xls = True              ham : spam   =    281.8 : 1.0\n",
      "                 stinson = True              ham : spam   =    267.4 : 1.0\n",
      "                crenshaw = True              ham : spam   =    251.5 : 1.0\n",
      "                     ect = True              ham : spam   =    231.2 : 1.0\n",
      "                   corel = True             spam : ham    =    220.5 : 1.0\n",
      "              macromedia = True             spam : ham    =    210.9 : 1.0\n",
      "              scheduling = True              ham : spam   =    209.6 : 1.0\n",
      "                       \u0001 = True              ham : spam   =    184.1 : 1.0\n",
      "                     sex = True             spam : ham    =    182.3 : 1.0\n",
      "                      xp = True             spam : ham    =    172.6 : 1.0\n",
      "                   daren = True              ham : spam   =    168.7 : 1.0\n",
      "                    1933 = True             spam : ham    =    152.1 : 1.0\n",
      "                    spam = True             spam : ham    =    145.1 : 1.0\n",
      "                 parsing = True              ham : spam   =    137.6 : 1.0\n",
      "                   penis = True             spam : ham    =    117.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clasify the below as spam or ham\n",
    "\n",
    "# Hint: 1. Break into words using word_tokenzise\n",
    "# 2. create_word_features\n",
    "# 3. Use the classify function\n",
    "\n",
    "msg1 = '''Hello th̓ere seُx master :-)\n",
    "i need c0ck ri͏ght noِw ..͏. don't tell my hǔbbٚy.ٚ. ))\n",
    "My sc͕rٞeٚe̻nname is Dorry.\n",
    "My accֺo֔unt is h֯ere: http:nxusxbnd.GirlsBadoo.ru\n",
    "C u late٘r!'''\n",
    "\n",
    "\n",
    "msg2 = '''As one of our top customers we are providing 10% OFF the total of your next used book purchase from www.letthestoriesliveon.com. Please use the promotional code, TOPTENOFF at checkout. Limited to 1 use per customer. All books have free shipping within the contiguous 48 United States and there is no minimum purchase.\n",
    "\n",
    "We have millions of used books in stock that are up to 90% off MRSP and add tens of thousands of new items every day. Don’t forget to check back frequently for new arrivals.'''\n",
    "\n",
    "\n",
    "\n",
    "msg3 = '''To start off, I have a 6 new videos + transcripts in the members section. In it, we analyse the Enron email dataset, half a million files, spread over 2.5GB. It's about 1.5 hours of  video.\n",
    "\n",
    "I have also created a Conda environment for running the code (both free and member lessons). This is to ensure everyone is running the same version of libraries, preventing the Works on my machine problems. If you get a second, do you mind trying it here?'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 1 is : spam\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(msg1)\n",
    "features = create_word_features(words)\n",
    "print(\"Message 1 is :\" ,classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 2 is : spam\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(msg2)\n",
    "features = create_word_features(words)\n",
    "print(\"Message 2 is :\" ,classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 3 is : ham\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(msg3)\n",
    "features = create_word_features(words)\n",
    "print(\"Message 3 is :\" ,classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
