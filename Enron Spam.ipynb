{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': True, 'brown': True, 'fox': True, 'quick': True, 'the': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_word_features(words):\n",
    "    my_dict = dict([(word, True) for word in words])\n",
    "    return my_dict\n",
    "\n",
    "create_word_features([\"the\", \"quick\", \"brown\", \"quick\", \"a\", \"fox\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam ['enron1', 'enron2', 'enron3', 'enron4', 'enron5', 'enron6'] 0\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron1 ['ham', 'spam'] 1\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron1\\ham [] 3672\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron1\\spam [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron2 ['ham', 'spam'] 1\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron2\\ham [] 4361\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron2\\spam [] 1496\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron3 ['ham', 'spam'] 1\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron3\\ham [] 4012\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron3\\spam [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron4 ['ham', 'spam'] 1\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron4\\ham [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron4\\spam [] 4500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron5 ['ham', 'spam'] 1\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron5\\ham [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron5\\spam [] 3675\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron6 ['ham', 'spam'] 1\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron6\\ham [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron6\\spam [] 4500\n"
     ]
    }
   ],
   "source": [
    "rootdir = \"C:\\\\Users\\\\Shantnu\\\\Desktop\\\\Data Sources\\\\Enron Spam\"\n",
    "for directory, subdirectory, filenames in  os.walk(rootdir):\n",
    "    print(directory, subdirectory, len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.path.split(\"C:\\\\Users\\\\Shantnu\\\\Desktop\\\\Data Sources\\\\Enron Spam\\\\enron1\\\\ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron1\\ham [] 3672\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron1\\spam [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron2\\ham [] 4361\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron2\\spam [] 1496\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron3\\ham [] 4012\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron3\\spam [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron4\\ham [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron4\\spam [] 4500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron5\\ham [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron5\\spam [] 3675\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron6\\ham [] 1500\n",
      "C:\\Users\\Shantnu\\Desktop\\Data Sources\\Enron Spam\\enron6\\spam [] 4500\n"
     ]
    }
   ],
   "source": [
    "rootdir = \"C:\\\\Users\\\\Shantnu\\\\Desktop\\\\Data Sources\\\\Enron Spam\"\n",
    "ham_list = []\n",
    "spam_list = []\n",
    "\n",
    "for directory, subdirectory, filenames in  os.walk(rootdir):\n",
    "    if (os.path.split(directory)[1]) == 'ham':\n",
    "        print(directory, subdirectory, len(filenames))\n",
    "        for filename in filenames:\n",
    "            with open(os.path.join(directory, filename), \"r\") as f:\n",
    "\n",
    "                data = f.read()\n",
    "            words = word_tokenize(data)\n",
    "            ham_list.append((create_word_features(words), \"ham\"))\n",
    "        \n",
    "    if (os.path.split(directory)[1]) == 'spam':\n",
    "        print(directory, subdirectory, len(filenames))\n",
    "        for filename in filenames:\n",
    "            #print(\"Opening file \", filename)\n",
    "            with open(os.path.join(directory, filename), encoding=\"latin-1\") as f:\n",
    "\n",
    "                data = f.read()\n",
    "            words = word_tokenize(data)\n",
    "            spam_list.append((create_word_features(words), \"spam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16545\n",
      "17171\n",
      "33716\n"
     ]
    }
   ],
   "source": [
    "print(len(ham_list))\n",
    "print(len(spam_list))\n",
    "\n",
    "combined_list = ham_list + spam_list\n",
    "\n",
    "print(len(combined_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://python-notes.curiousefficiency.org/en/latest/python3/text_file_processing.html#text-file-processing\n",
    "'''    \n",
    "with open(file1, encoding=\"latin-1\") as f:\n",
    "    data = f.read()\n",
    "print(data)    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23601\n",
      "10115\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(combined_list) * .7)\n",
    "\n",
    "train_data = combined_list[:train_size]\n",
    "test_data = combined_list[train_size:]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.46762234305487\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.util.accuracy(classifier, test_data)\n",
    "print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   enron = True              ham : spam   =   3674.4 : 1.0\n",
      "                     php = True             spam : ham    =    417.5 : 1.0\n",
      "                    meds = True             spam : ham    =    294.9 : 1.0\n",
      "                 stinson = True              ham : spam   =    279.6 : 1.0\n",
      "                     713 = True              ham : spam   =    268.9 : 1.0\n",
      "                     hpl = True              ham : spam   =    247.9 : 1.0\n",
      "                     ect = True              ham : spam   =    228.4 : 1.0\n",
      "              scheduling = True              ham : spam   =    215.5 : 1.0\n",
      "                   daren = True              ham : spam   =    214.3 : 1.0\n",
      "             medications = True             spam : ham    =    206.7 : 1.0\n",
      "              macromedia = True             spam : ham    =    203.5 : 1.0\n",
      "             derivatives = True              ham : spam   =    194.2 : 1.0\n",
      "                       \u0001 = True              ham : spam   =    185.1 : 1.0\n",
      "                  louise = True              ham : spam   =    177.0 : 1.0\n",
      "                    pill = True             spam : ham    =    161.0 : 1.0\n",
      "                 parsing = True              ham : spam   =    145.0 : 1.0\n",
      "               schedules = True              ham : spam   =    143.4 : 1.0\n",
      "                     mrs = True             spam : ham    =    130.5 : 1.0\n",
      "                    omit = True             spam : ham    =    124.1 : 1.0\n",
      "                     nom = True              ham : spam   =    123.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello th̓ere seُx master :-)\n",
      "i need c0ck ri͏ght noِw ..͏. don't tell my hǔbbٚy.ٚ. ))\n",
      "My sc͕rٞeٚe̻nname is Dorry.\n",
      "My accֺo֔unt is h֯ere: http:nxusxbnd.GirlsBadoo.ru\n",
      "C u late٘r!\n"
     ]
    }
   ],
   "source": [
    "spam_msg = '''Hello th̓ere seُx master :-)\n",
    "i need c0ck ri͏ght noِw ..͏. don't tell my hǔbbٚy.ٚ. ))\n",
    "My sc͕rٞeٚe̻nname is Dorry.\n",
    "My accֺo֔unt is h֯ere: http:nxusxbnd.GirlsBadoo.ru\n",
    "C u late٘r!'''\n",
    "\n",
    "print(spam_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(spam_msg)\n",
    "words = create_word_features(words)\n",
    "classifier.classify(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_msg2 = '''As one of our top customers we are providing 10% OFF the total of your next used book purchase from www.letthestoriesliveon.com. Please use the promotional code, TOPTENOFF at checkout. Limited to 1 use per customer. All books have free shipping within the contiguous 48 United States and there is no minimum purchase.\n",
    "\n",
    " \n",
    "\n",
    "We have millions of used books in stock that are up to 90% off MRSP and add tens of thousands of new items every day. Don’t forget to check back frequently for new arrivals.'''\n",
    "\n",
    "words = word_tokenize(spam_msg2)\n",
    "words = create_word_features(words)\n",
    "classifier.classify(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_msg3 = '''To start off, I have a 6 new videos + transcripts in the members section. In it, we analyse the Enron email dataset, half a million files, spread over 2.5GB. It's about 1.5 hours of  video.\n",
    "\n",
    "I have also created a Conda environment for running the code (both free and member lessons). This is to ensure everyone is running the same version of libraries, preventing the Works on my machine problems. If you get a second, do you mind trying it here?'''\n",
    "\n",
    "words = word_tokenize(spam_msg3)\n",
    "words = create_word_features(words)\n",
    "classifier.classify(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
